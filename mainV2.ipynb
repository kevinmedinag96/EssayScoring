{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"/content/drive/MyDrive/MCC-i cuarto semestre/Data Analytics/Project/\"\n",
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"torch==2.2.2\" tensorboard\n",
    "!pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\" peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import f_oneway\n",
    "import re\n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from packaging import version\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, cohen_kappa_score, balanced_accuracy_score, accuracy_score, roc_auc_score,r2_score\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, TaskType\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    DefaultDataCollator,\n",
    "    DataCollatorForLanguageModeling \n",
    "    \n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "from typing import Dict, List, NamedTuple, Optional, Tuple, Union\n",
    "\n",
    "from transformers.deepspeed import deepspeed_init\n",
    "\n",
    "from transformers.file_utils import (\n",
    "    CONFIG_NAME,\n",
    "    WEIGHTS_NAME,\n",
    "    PushToHubMixin,\n",
    "    is_apex_available,\n",
    "    is_datasets_available,\n",
    "    is_in_notebook,\n",
    "    is_sagemaker_dp_enabled,\n",
    "    is_sagemaker_mp_enabled,\n",
    "    is_training_run_on_sagemaker,\n",
    ")\n",
    "\n",
    "if is_sagemaker_mp_enabled():\n",
    "    from transformers.trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_torch_generator_available = True\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "from transformers.trainer_pt_utils import (\n",
    "    DistributedLengthGroupedSampler,\n",
    "    DistributedSamplerWithLoop,\n",
    "    DistributedTensorGatherer,\n",
    "    IterableDatasetShard,\n",
    "    LabelSmoother,\n",
    "    LengthGroupedSampler,\n",
    "    SequentialDistributedSampler,\n",
    "    ShardSampler,\n",
    "    distributed_broadcast_scalars,\n",
    "    distributed_concat,\n",
    "    find_batch_size,\n",
    "    get_parameter_names,\n",
    "    nested_concat,\n",
    "    nested_detach,\n",
    "    nested_numpify,\n",
    "    nested_truncate,\n",
    "    nested_xla_mesh_reduce,\n",
    "    reissue_pt_warnings,\n",
    ")\n",
    "\n",
    "from transformers.trainer_utils import (\n",
    "    PREFIX_CHECKPOINT_DIR,\n",
    "    BestRun,\n",
    "    EvalLoopOutput,\n",
    "    EvalPrediction,\n",
    "    HPSearchBackend,\n",
    "    PredictionOutput,\n",
    "    TrainerMemoryTracker,\n",
    "    TrainOutput,\n",
    "    default_compute_objective,\n",
    "    denumpify_detensorize,\n",
    "    get_last_checkpoint,\n",
    "    number_of_arguments,\n",
    "    set_seed,\n",
    "    speed_metrics,\n",
    ")\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to Hugging face (required only for Llama 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Essay Score Dataset & perfom NLP Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/train.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "data_path = \"data/test.csv\"\n",
    "df_test = pd.read_csv(data_path)\n",
    "\n",
    "data_features = \"data/features_df.csv\"\n",
    "df_features = pd.read_csv(data_features).drop([\"score\",\"Unnamed: 0\"],axis=1)\n",
    "col_df_features = df_features.columns.to_list()\n",
    "\n",
    "#concat text with new textstat,linguistic, and mispelling features\n",
    "df = pd.concat([df,df_features],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"score\",\"essay_id\"])\n",
    "y = df[\"score\"].astype(float)\n",
    "test = df_test.drop(columns=[\"essay_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = y.value_counts(normalize=True).sort_index().to_list()\n",
    "class_weights = torch.Tensor(class_weights)\n",
    "class_weights=(1 - class_weights) / (1 - class_weights).sum()\n",
    "print(class_weights)\n",
    "\n",
    "labels_arr = np.arange(class_weights.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHTML(x):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',x)\n",
    "\n",
    "\n",
    "cList = {#\"dont\" : \"do not\", \"doesnt\" : \"does not\", \"thats\" : \"that is\"\n",
    "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "    \"he'd\": \"he would\",  ## --> he had or he would\n",
    "    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n",
    "    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",   ## --> I had or I would\n",
    "    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",   ## --> It had or It would\n",
    "    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",   ## --> It had or It would\n",
    "    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n",
    "    \"you're\": \"you are\",  \"you've\": \"you have\"\n",
    "}\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "def dataPreprocessing(x):\n",
    "    # Convert words to lowercase\n",
    "    x = x.lower()\n",
    "    # Remove HTML\n",
    "    x = removeHTML(x)\n",
    "    # Delete strings starting with @\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    # Delete Numbers\n",
    "    #x = re.sub(\"'\\d+\", '',x)\n",
    "    #x = re.sub(\"\\d+\", '',x)\n",
    "    # Delete URL\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    # Remove \\xa0\n",
    "    x = x.replace(u'\\xa0',' ')\n",
    "    # Replace consecutive empty spaces with a single space character\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = expandContractions(x)\n",
    "    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "#     x = re.sub(r'[^\\w\\s.,;:\"\"''?!]', '', x)\n",
    "  #replace \\'s with 's\n",
    "    x = re.sub(r\"\\'\", \"'\", x)\n",
    "    \n",
    "    # Remove empty characters at the beginning and end\n",
    "    x = x.strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_pipeline(text):\n",
    "    doc = nltk.word_tokenize(text)\n",
    "    text = \" \".join(token for token in doc)\n",
    "\n",
    "    def delete_extra_spacing(text):\n",
    "        # Reemplaza \" . \" con \". \"\n",
    "        text = re.sub(r'\\s*\\.\\s*', '. ', text)\n",
    "        # Reemplaza dos o mÃ¡s espacios con un solo espacio\n",
    "        text = re.sub(r'\\s{2,}', ' ', text)\n",
    "        return text\n",
    "\n",
    "    text = delete_extra_spacing(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "res = X[\"full_text\"][:10].apply(nlp_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_apostrofes(cadena):\n",
    "    # Reemplaza todas las ocurrencias de \\'\n",
    "    nueva_cadena = re.sub(r\"\\'\", \"'\", cadena)\n",
    "    return nueva_cadena\n",
    "\n",
    "mi_cadena = \"Este es un ejemplo con\\'s en la cadena.\"\n",
    "resultado = reemplazar_apostrofes(mi_cadena)\n",
    "print(resultado)  # Salida: \"Este es un ejemplo con ' en la cadena.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"full_text\"] = X.apply(lambda x: dataPreprocessing(x[\"full_text\"]),axis=1)\n",
    "test[\"full_text\"] = test.apply(lambda x: dataPreprocessing(x[\"full_text\"]),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train-Validation Pandas Dataset Split Note: Huggingface trainer needs the datasets to be compatible with huggingface dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 10\n",
    "generator = np.random.RandomState(seed)\n",
    "df_size = len(X)\n",
    "train_proportion = 0.85\n",
    "validation_proportion = 0.15\n",
    "train_size = int(df_size * train_proportion)\n",
    "validation_size = df_size - train_size \n",
    "arr_train_idxs = generator.choice(np.arange(len(X)),size=[train_size,],replace=False)\n",
    "\n",
    "train = X.iloc[arr_train_idxs].reset_index(drop=True)\n",
    "train_labels = y.iloc[arr_train_idxs].reset_index(drop=True)\n",
    "validation = X[~X.index.isin(arr_train_idxs)].reset_index(drop=True)\n",
    "validation_labels = y.iloc[~y.index.isin(arr_train_idxs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concat = pd.concat([train,train_labels],axis=1)\n",
    "validation_concat =pd.concat([validation,validation_labels],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas dataframe datasets to huggingface datasets objects...\n",
    "dataset_train = Dataset.from_pandas(train_concat)\n",
    "dataset_val = Dataset.from_pandas(validation_concat)\n",
    "dataset_test = Dataset.from_pandas(test)\n",
    "\n",
    "# Combine them into a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    'val': dataset_val\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Model for Essay Score Assesment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_path = \"tests/DeBERTaV3-No-Qlora\"\n",
    "log_vm_path = \"tests/DeBERTaV3-No-Qlora/log/session1\"\n",
    "config = {\n",
    "    'model': \"microsoft/deberta-v3-base\",\n",
    "    \"model_dir\" : vm_path,\n",
    "    'log_dir' : log_vm_path,\n",
    "    'max_length': 1900,\n",
    "    'batch_size': 15, \n",
    "    'epochs': 20,\n",
    "    'freeze_encoder': True,\n",
    "    'hidden_dim' : 1024,\n",
    "    'out_dim' : 1,\n",
    "    'eng_feats_dim' :len(df_features.columns.to_list()),\n",
    "    'backbone_out_dim' : class_weights.shape[0],\n",
    "    'lr': 1e-4,\n",
    "    'enable_scheduler': True,\n",
    "    'scheduler': 'CosineAnnealingWarmRestarts',\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'adam_eps': 1e-6, # 1e-8 default\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config[\"model\"],\n",
    "    #quantization_config=quantization_config,\n",
    "    num_labels=class_weights.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#model.resize_position_embeddings(config[\"max_length\"])\n",
    "\n",
    "#for param in model.pre_classifier.parameters():\n",
    "#    param.requires_grad = True\n",
    "\n",
    "for param in base_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "  \n",
    "for param in base_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "#base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct custom model with backbone encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg ={\n",
    "    \"id2label\": {\n",
    "    \"0\": \"LABEL_0\",\n",
    "    \"1\": \"LABEL_1\",\n",
    "    \"2\": \"LABEL_2\",\n",
    "    \"3\": \"LABEL_3\",\n",
    "    \"4\": \"LABEL_4\",\n",
    "    \"5\": \"LABEL_5\"\n",
    "  } ,\n",
    "\"label2id\": {\n",
    "    \"LABEL_0\": 0,\n",
    "    \"LABEL_1\": 1,\n",
    "    \"LABEL_2\": 2,\n",
    "    \"LABEL_3\": 3,\n",
    "    \"LABEL_4\": 4,\n",
    "    \"LABEL_5\": 5\n",
    "  } \n",
    "} \n",
    "\n",
    "class CustomConfig(PretrainedConfig):\n",
    "    model_type = \"DeBERTaV3\"\n",
    "    def __init__(self,model_name = None,freeze_encoder = None,\n",
    "        backbone_out_dim = None,eng_feats_dim = None,hidden_dim = None,out_dim = None,**kwargs):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        self.eng_feats_dim = eng_feats_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.backbone_out_dim = backbone_out_dim\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "hf_config = CustomConfig(config[\"model\"],config[\"freeze_encoder\"],config[\"backbone_out_dim\"],config[\"eng_feats_dim\"],\n",
    "config[\"hidden_dim\"],config[\"out_dim\"] )\n",
    "#config.save_pretrained(\"./custom-debertav3\")\n",
    "#AutoConfig.register(\"deberta_custom\", DebertaV2Config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(PreTrainedModel):\n",
    "    base_model_prefix = \"encoder\"\n",
    "    def __init__(self,config) -> None:\n",
    "        self.config = config\n",
    "        super().__init__(self.config)\n",
    "        #self.base_model = base_model\n",
    "        self.encoder = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.config.model_name,\n",
    "            #quantization_config=quantization_config,\n",
    "            #self.config=config\n",
    "            num_labels=self.config.backbone_out_dim\n",
    "        )\n",
    "\n",
    "        self.freeze_backbone()\n",
    "\n",
    "        self.final_classifier = nn.Sequential(\n",
    "            nn.Linear(self.config.backbone_out_dim + self.config.eng_feats_dim,self.config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.config.hidden_dim, self.config.out_dim )\n",
    "        )\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        if self.config.freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            for param in self.encoder.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            for param in self.encoder.pooler.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def print_log(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            print(param)\n",
    "\n",
    "\n",
    "    def forward(self,**kwargs):\n",
    "        #utilize encoder as feature extractor to obtain logits of size class num\n",
    "        eng_features = kwargs.pop(\"eng_features\")\n",
    "\n",
    "        outputs_encoder = self.base_model(**kwargs)\n",
    "        x = outputs_encoder.get(\"logits\")\n",
    "\n",
    "        #concat encoder features + eng features\n",
    "        x = torch.cat([x,eng_features],dim=-1)\n",
    "\n",
    "        #forward pass to the classifier layer\n",
    "        x = self.final_classifier(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel.from_pretrained(hf_config.model_name,config=hf_config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model\"], add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the tokenizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train[\"full_text\"][1]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(text)\n",
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(encoded) \n",
    "decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model tokenized dataset from huggingface dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['full_text'], truncation=True,padding=\"max_length\" ,max_length=config[\"max_length\"])\n",
    "\n",
    "cols_to_delete = [\"full_text\"]\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True,remove_columns=cols_to_delete)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"score\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Model Forward pass & Quadratic Weighted Kappa score test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokenize_function(examples):\n",
    "    return tokenizer(examples['full_text'], truncation=True,padding=\"max_length\", max_length=config[\"max_length\"])\n",
    "\n",
    "cols_to_delete = [\"full_text\"]\n",
    "\n",
    "tokenized_datasets_pad = dataset.map(pad_tokenize_function, batched=True,remove_columns=cols_to_delete)\n",
    "tokenized_datasets_pad = tokenized_datasets_pad.rename_column(\"score\", \"label\")\n",
    "tokenized_datasets_pad.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenized_datasets_pad[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_split_fn(batch):\n",
    "    \"\"\"\n",
    "    :batch: dict of tensors for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    eng_feat_tensor = torch.tensor([  batch[col].tolist() for col in col_df_features],dtype=torch.float32).transpose(0,1).to(device)\n",
    "\n",
    "    inputs = {\"input_ids\" : torch.tensor([sample.tolist() for sample in batch[\"input_ids\"]],dtype= torch.long).to(device),\n",
    "    \"token_type_ids\" : torch.tensor([sample.tolist() for sample in batch[\"token_type_ids\"]],dtype= torch.long).to(device),\n",
    "    \"attention_mask\" : torch.tensor([sample.tolist() for sample in batch[\"attention_mask\"]],dtype= torch.long).to(device)\n",
    "    }\n",
    "\n",
    "    del batch\n",
    "\n",
    "\n",
    "    return inputs, eng_feat_tensor, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, eng_features, labels = input_split_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(eng_features,**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs[:,0].clip(0,class_weights.shape[0] - 1).round().detach().cpu()\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.detach().cpu()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(y1=logits,y2=labels,labels=labels_arr,weights=\"quadratic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data collator prepares batches of data for training or inference in machine learning, ensuring uniform formatting and adherence to model input requirements. This is especially crucial for variable-sized inputs like text sequences.\n",
    "\n",
    "Functions of Data Collator\n",
    "\n",
    "1.- Padding: Uniformly pads sequences to the length of the longest sequence using a special token, allowing simultaneous batch processing.\n",
    "\n",
    "2.- Batching: Groups individual data points into batches for efficient processing.\n",
    "\n",
    "3.- Handling Special Tokens: Adds necessary special tokens to sequences.\n",
    "\n",
    "4.- Converting to Tensor: Transforms data into tensors, the required format for machine learning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer,max_length=config[\"max_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions[:,0]\n",
    "\n",
    "    r2 = r2_score(labels,predictions)\n",
    "\n",
    "    predictions = predictions.clip(1,class_weights.shape[0]).round()\n",
    "\n",
    "    qwk = cohen_kappa_score(y1=predictions,y2=labels,labels=labels_arr,weights=\"quadratic\")\n",
    "    acc =accuracy_score(labels,predictions)\n",
    "    balanced_acc = balanced_accuracy_score(labels,predictions)\n",
    "\n",
    "    return  {\n",
    "        \"r2\" : r2,\n",
    "        \"QWK\" : qwk,\n",
    "        \"acc\" : acc,\n",
    "        \"balanced acc\" : balanced_acc}\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = class_weights\n",
    "            if not isinstance(self.class_weights, torch.FloatTensor):\n",
    "                self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "            self.class_weights = self.class_weights.to(device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "\n",
    "    def evaluate(\n",
    "            self,\n",
    "            eval_dataset = None,\n",
    "            ignore_keys = None,\n",
    "            metric_key_prefix: str = \"eval\",\n",
    "        ): \n",
    "            # memory metrics - must set up as early as possible\n",
    "            self._memory_tracker.start()\n",
    "\n",
    "            eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "            start_time  = time.time()\n",
    "\n",
    "            eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
    "\n",
    "            output = eval_loop(\n",
    "            eval_dataloader,\n",
    "            # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
    "            # self.args.prediction_loss_only\n",
    "            prediction_loss_only=True if self.compute_metrics is None else None,\n",
    "            ignore_keys=ignore_keys,\n",
    "            metric_key_prefix=metric_key_prefix,\n",
    "            )\n",
    "\n",
    "            total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "            output.metrics.update(\n",
    "                speed_metrics(\n",
    "                    metric_key_prefix,\n",
    "                    start_time,\n",
    "                    num_samples=output.num_samples,\n",
    "                    num_steps=math.ceil(output.num_samples / total_batch_size),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.log(output.metrics)\n",
    "\n",
    "            #if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n",
    "            #    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "            #    xm.master_print(met.metrics_report())\n",
    "\n",
    "            self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)\n",
    "\n",
    "            self._memory_tracker.stop_and_update_metrics(output.metrics)\n",
    "\n",
    "            return output.metrics\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "    def evaluation_loop(self,dataloader : DataLoader,prediction_loss_only: Optional[bool] = None,\n",
    "                        ignore_keys : Optional[List[str]] = None, metric_key_prefix : str = \"eval\"):\n",
    "        \n",
    "        prediction_loss_only = (\n",
    "            prediction_loss_only if prediction_loss_only is not None else self.args.prediction_loss_only\n",
    "        )\n",
    "\n",
    "\n",
    "        # if eval is called w/o train init deepspeed here\n",
    "        if self.args.deepspeed and not self.deepspeed:\n",
    "\n",
    "            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n",
    "            # from the checkpoint eventually\n",
    "            deepspeed_engine, _, _ = deepspeed_init(self, num_training_steps=0, resume_from_checkpoint=None)\n",
    "            self.model = deepspeed_engine.module\n",
    "            self.model_wrapped = deepspeed_engine\n",
    "            self.deepspeed = deepspeed_engine\n",
    "            # XXX: we don't need optim/sched for inference, but this needs to be sorted out, since\n",
    "            # for example the Z3-optimizer is a must for zero3 to work even for inference - what we\n",
    "            # don't need is the deepspeed basic optimizer which is self.optimizer.optimizer\n",
    "            deepspeed_engine.optimizer.optimizer = None\n",
    "            deepspeed_engine.lr_scheduler = None\n",
    "\n",
    "        \n",
    "        model = self._wrap_model(self.model, training=False)\n",
    "\n",
    "        # if full fp16 is wanted on eval and this ``evaluation`` or ``predict`` isn't called while\n",
    "        # ``train`` is running, halve it first and then put on device\n",
    "        if not self.is_in_train and self.args.fp16_full_eval:\n",
    "            model = model.half().to(self.args.device)\n",
    "\n",
    "        batch_size = config[\"batch_size\"]\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        self.callback_handler.eval_dataloader = dataloader\n",
    "\n",
    "        # Do this before wrapping.\n",
    "        eval_dataset = dataloader.dataset\n",
    "\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = None\n",
    "\n",
    "        # Initialize containers\n",
    "        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n",
    "        losses_host = None\n",
    "        preds_host = None\n",
    "        labels_host = None\n",
    "        # losses/preds/labels on CPU (final containers)\n",
    "        all_losses = None\n",
    "        all_preds = None\n",
    "        all_labels = None\n",
    "        # Will be useful when we have an iterable dataset so don't know its length.\n",
    "\n",
    "\n",
    "        observed_num_examples = 0\n",
    "        # Main evaluation loop\n",
    "        input_dic = {}\n",
    "        for step, inputs in enumerate(dataloader):\n",
    "            # Update the observed num examples\n",
    "            observed_batch_size = find_batch_size(inputs)\n",
    "\n",
    "            if observed_batch_size is not None:\n",
    "                observed_num_examples += observed_batch_size\n",
    "            # Update the observed num examples\n",
    "            #split input\n",
    "            inputs, eng_features, class_labels = input_split_fn(inputs)\n",
    "            #inputs, class_labels = inputs.get('input_ids'),inputs.get('labels')\n",
    "            input_dic.update({\n",
    "                k : v for k,v in inputs.items()\n",
    "            })\n",
    "\n",
    "            input_dic.update({\n",
    "                \"eng_features\" : eng_features\n",
    "            })\n",
    "\n",
    "            input_dic.update({\n",
    "                \"labels\" : class_labels\n",
    "            })\n",
    "\n",
    "            # Prediction step\n",
    "            loss, logits, labels = self.prediction_step(model, input_dic, prediction_loss_only, ignore_keys=ignore_keys)\n",
    "\n",
    "            # Update containers on host\n",
    "            if loss is not None:\n",
    "                losses = self._nested_gather(loss.repeat(batch_size))\n",
    "                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n",
    "            if logits is not None:\n",
    "                #logits = self._pad_across_processes(logits)\n",
    "                logits = self._nested_gather(logits)\n",
    "                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n",
    "            if labels is not None:\n",
    "                #labels = self._pad_across_processes(labels)\n",
    "                labels = self._nested_gather(labels)\n",
    "                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n",
    "            self.control = self.callback_handler.on_prediction_step(self.args, self.state, self.control)\n",
    "\n",
    "            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n",
    "            if self.args.eval_accumulation_steps is not None and (step + 1) % self.args.eval_accumulation_steps == 0:\n",
    "                if losses_host is not None:\n",
    "                    losses = nested_numpify(losses_host)\n",
    "                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
    "                if preds_host is not None:\n",
    "                    logits = nested_numpify(preds_host)\n",
    "                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
    "                if labels_host is not None:\n",
    "                    labels = nested_numpify(labels_host)\n",
    "                    all_labels = (\n",
    "                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
    "                    )\n",
    "\n",
    "                # Set back to None to begin a new accumulation\n",
    "                losses_host, preds_host, labels_host = None, None, None\n",
    "\n",
    "        if self.args.past_index and hasattr(self, \"_past\"):\n",
    "            # Clean the state at the end of the evaluation loop\n",
    "            delattr(self, \"_past\")\n",
    "\n",
    "        # Gather all remaining tensors and put them back on the CPU\n",
    "        if losses_host is not None:\n",
    "            losses = nested_numpify(losses_host)\n",
    "            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n",
    "        if preds_host is not None:\n",
    "            logits = nested_numpify(preds_host)\n",
    "            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
    "        if labels_host is not None:\n",
    "            labels = nested_numpify(labels_host)\n",
    "            all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n",
    "\n",
    "        # Number of samples\n",
    "        if not isinstance(eval_dataset, IterableDataset):\n",
    "            num_samples = len(eval_dataset)\n",
    "        # The instance check is weird and does not actually check for the type, but whether the dataset has the right\n",
    "        # methods. Therefore we need to make sure it also has the attribute.\n",
    "        elif isinstance(eval_dataset, IterableDatasetShard) and hasattr(eval_dataset, \"num_examples\"):\n",
    "            num_samples = eval_dataset.num_examples\n",
    "        else:\n",
    "            num_samples = observed_num_examples\n",
    "\n",
    "        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n",
    "        # samplers has been rounded to a multiple of batch_size, so we truncate.\n",
    "        if all_losses is not None:\n",
    "            pass\n",
    "            #all_losses = all_losses[:num_samples]\n",
    "        if all_preds is not None:\n",
    "            all_preds = nested_truncate(all_preds, num_samples)\n",
    "        if all_labels is not None:\n",
    "            all_labels = nested_truncate(all_labels, num_samples)\n",
    "\n",
    "        # Metrics!\n",
    "        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n",
    "            metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n",
    "        else:\n",
    "            metrics = {}\n",
    "\n",
    "        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n",
    "        metrics = denumpify_detensorize(metrics)\n",
    "\n",
    "        if all_losses is not None:\n",
    "            metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n",
    "\n",
    "        # Prefix all keys with metric_key_prefix + '_'\n",
    "        for key in list(metrics.keys()):\n",
    "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "\n",
    "        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)\n",
    "\n",
    "            \n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs,\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on :obj:`model` using obj:`inputs`.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (:obj:`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "            ignore_keys (:obj:`Lst[str]`, `optional`):\n",
    "                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
    "                gathering predictions.\n",
    "\n",
    "        Return:\n",
    "            Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
    "            logits and labels (each being optional).\n",
    "        \"\"\"\n",
    "        has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        if ignore_keys is None:\n",
    "            if hasattr(self.model, \"config\"):\n",
    "                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "            else:\n",
    "                ignore_keys = []\n",
    "\n",
    "        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
    "        if has_labels:\n",
    "            labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n",
    "            if len(labels) == 1:\n",
    "                labels = labels[0]\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if is_sagemaker_mp_enabled():\n",
    "                raw_outputs = smp_forward_only(model, inputs)\n",
    "                if has_labels:\n",
    "                    if isinstance(raw_outputs, dict):\n",
    "                        loss_mb = raw_outputs[\"loss\"]\n",
    "                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys + [\"loss\"])\n",
    "                    else:\n",
    "                        loss_mb = raw_outputs[0]\n",
    "                        logits_mb = raw_outputs[1:]\n",
    "\n",
    "                    loss = loss_mb.reduce_mean().detach().cpu()\n",
    "                    logits = smp_nested_concat(logits_mb)\n",
    "                else:\n",
    "                    loss = None\n",
    "                    if isinstance(raw_outputs, dict):\n",
    "                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys)\n",
    "                    else:\n",
    "                        logits_mb = raw_outputs\n",
    "                    logits = smp_nested_concat(logits_mb)\n",
    "            else:\n",
    "                if has_labels:\n",
    "                    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "                    loss = loss.mean().detach()\n",
    "                    if isinstance(outputs, dict):\n",
    "                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])\n",
    "                    else:\n",
    "                        logits = outputs\n",
    "                else:\n",
    "                    loss = None\n",
    "                    if self.use_amp:\n",
    "                        with autocast():\n",
    "                            outputs = model(**inputs)\n",
    "                    else:\n",
    "                        outputs = model(**inputs)\n",
    "                    if isinstance(outputs, dict):\n",
    "                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n",
    "                    else:\n",
    "                        logits = outputs\n",
    "                    # TODO: this needs to be fixed and made cleaner later.\n",
    "                    if self.args.past_index >= 0:\n",
    "                        self._past = outputs[self.args.past_index - 1]\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        logits = nested_detach(logits)\n",
    "        if len(logits) == 1:\n",
    "            logits = logits[0]\n",
    "\n",
    "        return (loss, logits, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        #inputs, eng_features, labels = input_split_fn(inputs)\n",
    "        #labels = inputs.pop(\"labels\")\n",
    "\n",
    "        model_inps = inputs\n",
    "\n",
    "        #get input , feat, label tensors\n",
    "        if \"eng_features\" not in inputs:\n",
    "            inputs, eng_features, labels = input_split_fn(inputs)\n",
    "            #inputs, class_labels = inputs.get('input_ids'),inputs.get('labels')\n",
    "            input_dic = {\n",
    "                    k : v for k,v in inputs.items()\n",
    "            }\n",
    "\n",
    "            input_dic.update({\n",
    "                    \"eng_features\" : eng_features\n",
    "                })\n",
    "            \n",
    "            model_inps = input_dic\n",
    "        else:\n",
    "            labels = model_inps.pop(\"labels\")\n",
    "\n",
    "        #compare first token essay to the dataset to find similarities\n",
    "        # Forward pass\n",
    "        outputs = model(**model_inps)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        #logits = outputs.get('logits')[:,0]\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            raise NotImplementedError(\"Class weights is deprecated in Regression problems\")\n",
    "\n",
    "        else:\n",
    "            loss = F.mse_loss(outputs,labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = config[\"model_dir\"],\n",
    "    logging_dir = config[\"log_dir\"],\n",
    "    learning_rate = config[\"lr\"],\n",
    "    per_device_train_batch_size = config[\"batch_size\"],\n",
    "    per_device_eval_batch_size = config[\"batch_size\"],\n",
    "    num_train_epochs = config[\"epochs\"],\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch', \n",
    "    #eval_steps=2,\n",
    "    save_strategy = 'epoch',\n",
    "    #save_steps = 8,\n",
    "    load_best_model_at_end = True,\n",
    "    save_total_limit=3,\n",
    "    logging_strategy = \"epoch\",\n",
    "    #logging_steps = 2,\n",
    "    data_seed = seed,\n",
    "    remove_unused_columns=False,\n",
    "    metric_for_best_model=\"QWK\",\n",
    "    seed = seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    #col_df_features=col_df_features,\n",
    "    #max_seq_len=None,\n",
    "    #class_weights=class_weights,\n",
    "    callbacks= [EarlyStoppingCallback(early_stopping_patience=3,early_stopping_threshold=.005),]\n",
    ")\n",
    "trainer.label_names.append(\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_sys_fashion_mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
