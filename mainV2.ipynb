{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import f_oneway\n",
    "import re\n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorBoardLogger:\n",
    "    def __init__(self, log_dir):\n",
    "        self.summary_writer = SummaryWriter(log_dir)\n",
    "\n",
    "    def add_scalars(self, tag_step_value_dict):\n",
    "        \"\"\"\n",
    "        :param parent_tag: str, e.g. \"Training Loss\"\n",
    "        :param tag_step_value_dict: dict, e.g., {\"key\":(step, value), \"q_grad\":(10000, 1.11)}\n",
    "        \"\"\"\n",
    "        for tag, (step, value) in tag_step_value_dict.items():\n",
    "            self.summary_writer.add_scalar(tag, value, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Essay Score Dataset & perfom NLP Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/train.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "data_path = \"data/test.csv\"\n",
    "df_test = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"score\",\"essay_id\"])\n",
    "y = df[\"score\"] - 1\n",
    "test = df_test.drop(columns=[\"essay_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHTML(x):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',x)\n",
    "\n",
    "\n",
    "cList = {#\"dont\" : \"do not\", \"doesnt\" : \"does not\", \"thats\" : \"that is\"\n",
    "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "    \"he'd\": \"he would\",  ## --> he had or he would\n",
    "    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n",
    "    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",   ## --> I had or I would\n",
    "    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",   ## --> It had or It would\n",
    "    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",   ## --> It had or It would\n",
    "    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n",
    "    \"you're\": \"you are\",  \"you've\": \"you have\"\n",
    "}\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "def dataPreprocessing(x):\n",
    "    # Convert words to lowercase\n",
    "    x = x.lower()\n",
    "    # Remove HTML\n",
    "    x = removeHTML(x)\n",
    "    # Delete strings starting with @\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    # Delete Numbers\n",
    "    #x = re.sub(\"'\\d+\", '',x)\n",
    "    #x = re.sub(\"\\d+\", '',x)\n",
    "    # Delete URL\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    # Remove \\xa0\n",
    "    x = x.replace(u'\\xa0',' ')\n",
    "    # Replace consecutive empty spaces with a single space character\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = expandContractions(x)\n",
    "    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "#     x = re.sub(r'[^\\w\\s.,;:\"\"''?!]', '', x)\n",
    "  #replace \\'s with 's\n",
    "    #print(re.findall(\"\\\\'s\", x))\n",
    "    #x = re.sub(r\"\\[\\]'s\", \"'s\", x)\n",
    "    # Remove empty characters at the beginning and end\n",
    "    x = x.strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"full_text\"] = X.apply(lambda x: dataPreprocessing(x[\"full_text\"]),axis=1)\n",
    "test[\"full_text\"] = test.apply(lambda x: dataPreprocessing(x[\"full_text\"]),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17307"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train-Validation Pandas Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 10\n",
    "generator = np.random.RandomState(seed)\n",
    "df_size = len(X)\n",
    "train_proportion = 0.8\n",
    "validation_proportion = 0.2\n",
    "train_size = int(df_size * train_proportion)\n",
    "validation_size = df_size - train_size \n",
    "arr_train_idxs = generator.choice(np.arange(1,len(X)),size=[train_size,],replace=False)\n",
    "\n",
    "train = X.iloc[arr_train_idxs].reset_index(drop=True)\n",
    "train_labels = y.iloc[arr_train_idxs].reset_index(drop=True)\n",
    "validation = X[~X.index.isin(arr_train_idxs)].reset_index(drop=True)\n",
    "validation_labels = y.iloc[~y.index.isin(arr_train_idxs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model': 'distilbert/distilbert-base-uncased',\n",
    "    'dropout': 0.2,\n",
    "    'max_length': 2048,\n",
    "    'batch_size': 10, # anything more results in CUDA OOM [for unfreezed encoder] on Kaggle GPU\n",
    "    'epochs': 10,\n",
    "    'lr': 3e-4,\n",
    "    'enable_scheduler': True,\n",
    "    'scheduler': 'CosineAnnealingWarmRestarts',\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'adam_eps': 1e-6, # 1e-8 default\n",
    "    'freeze_encoder': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinmg96/miniconda3/envs/unsloth/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train[\"full_text\"][0]\n",
    "\n",
    "encoded =tokenizer.encode(train[\"full_text\"][0])\n",
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] the face on mars is a landformation. people would argue it isnt but keep in mind that isnt the case with this. with people argueing that it isnt they think aliens did it but that may not have happend. the face of mars has to be a landformation. the face on mars is a landformation because natural occurances happen. this could have happend fromm a meteor strike or the winds in mars. not to mention if the clouds in mars are constant then finding this twice on over a 20 year period should still be the same fro slow winds and mild weater. many people belive aliens were the cause of the face. well thats nearly impossible if you consider the aliens were made in mars. life forms take upon millions of years to grow and need about 60 degree weather on average and mars has below freezing average. plus if theree were life on mars, there should be green on mars because of plants. people also think its aliens because of the lattitude changed. well this is because the camera that had to take this picture had to be on a cloudy storm probaly causeing some small problems but nothing minor. at the end i think the face is a landformation and that life could not of done this. it solid evidence. i mean stuff on earth happens like that all the time and people dont care, it is like how you see a bunny on the moon but in reality they are just cooled down oceans of lava. so the face on mars in my opinion is a landformation. [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded) \n",
    "decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Datasets & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayScoringDataset:\n",
    "    def __init__(self, df,y, config, tokenizer=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.y = y\n",
    "        self.max_len = config['max_length']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        essay = self.df.iloc[idx][\"full_text\"]\n",
    "        if self.y is not None:\n",
    "            score = self.y[idx]\n",
    "\n",
    "        tokenized = tokenizer.encode_plus(essay,\n",
    "                                          None,\n",
    "                                          add_special_tokens=True,\n",
    "                                          max_length=self.max_len,\n",
    "                                          truncation=True,\n",
    "                                          padding='max_length'\n",
    "                                         )\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(tokenized['input_ids'], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(tokenized['attention_mask'], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if self.is_test == True:\n",
    "            return inputs\n",
    "        \n",
    "        targets = {\n",
    "            \"labels\": torch.tensor(score).type(torch.LongTensor),\n",
    "        }\n",
    "        \n",
    "        return inputs, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pytorch_dataset = EssayScoringDataset(train,train_labels,config,tokenizer)\n",
    "validation_pytorch_dataset = EssayScoringDataset(validation,validation_labels,config,tokenizer)\n",
    "test_pytorch_dataset = EssayScoringDataset(validation,None,config,tokenizer,True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_pytorch_dataset, \n",
    "                                           batch_size=config[\"batch_size\"])\n",
    "valid_loader = torch.utils.data.DataLoader(validation_pytorch_dataset,batch_size=config[\"batch_size\"])\n",
    "test_loader = torch.utils.data.DataLoader(test_pytorch_dataset,\n",
    "                                          batch_size=config[\"batch_size\"])\n",
    "\n",
    "dataloaders = {\"train\" : [train_loader,len(train_pytorch_dataset)],\n",
    "               \"valid\" : [valid_loader,len(validation_pytorch_dataset)],\n",
    "               \"test\" : [test_loader,len(test_pytorch_dataset)]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test functionality of Dataloader\n",
    "demo_loader  = torch.utils.data.DataLoader(train_pytorch_dataset, batch_size=10)\n",
    "batch = next(iter(demo_loader))\n",
    "inputs, targets = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1012,  102,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pytorch_dataset[0][0][\"input_ids\"][320:330]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pytorch_dataset[0][0][\"attention_mask\"][320:330]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "\n",
    "class EssayModel(nn.Module):\n",
    "    def __init__(self,config,num_classes=6):\n",
    "        super().__init__()\n",
    "        self.model_name = config['model']\n",
    "        self.freeze = config['freeze_encoder']\n",
    "        \n",
    "        self.encoder = AutoModel.from_pretrained(self.model_name)\n",
    "\n",
    "        if self.freeze:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        #self.encoder.config.sinusoidal_pos_embds = True\n",
    "        #update word embeggins\n",
    "        self.encoder.resize_position_embeddings(config[\"max_length\"])\n",
    "        \n",
    "                \n",
    "        self.pooler = MeanPooling()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(self.encoder.config.hidden_size,num_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        outputs = self.encoder(**inputs,return_dict=True)\n",
    "        outputs = self.pooler(outputs['last_hidden_state'], inputs['attention_mask'])\n",
    "        outputs = self.classifier(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug initial weights embeddings : Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "mean_pool = MeanPooling()\n",
    "model = EssayModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(2048, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": true,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.40.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4534,  0.1980, -0.4353, -0.3093,  0.3490, -0.2494],\n",
       "        [-0.3071,  0.3186, -0.4425, -0.2849,  0.3202, -0.2637],\n",
       "        [-0.2170,  0.2765, -0.5082, -0.2564,  0.2611, -0.2908],\n",
       "        [-0.2299,  0.2738, -0.4787, -0.2624,  0.2848, -0.3053],\n",
       "        [-0.2243,  0.2747, -0.4809, -0.2583,  0.2825, -0.3033],\n",
       "        [-0.4028,  0.2675, -0.4281, -0.3060,  0.3106, -0.2215],\n",
       "        [-0.3590,  0.2798, -0.4298, -0.2614,  0.2992, -0.2684],\n",
       "        [-0.4054,  0.2576, -0.4156, -0.2985,  0.3222, -0.2269],\n",
       "        [-0.4530,  0.2027, -0.4426, -0.2947,  0.3536, -0.2563],\n",
       "        [-0.1992,  0.2072, -0.3722, -0.1664,  0.3606, -0.2773]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_loader  = torch.utils.data.DataLoader(train_pytorch_dataset, batch_size=10)\n",
    "batch = next(iter(demo_loader))\n",
    "inputs, targets = batch\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k : v.to(device) for k,v in inputs.items()}\n",
    "    targets = {k : v.to(device) for k,v in targets.items()}\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    outputs = model(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(gradient_accumulation_steps=config['gradient_accumulation_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model,model_dir, loaders, config, accelerator,logger : TensorBoardLogger,debug = False):\n",
    "        self.model_dir = model_dir\n",
    "        self.model = model\n",
    "        self.train_loader, self.val_loader = loaders\n",
    "        self.config = config\n",
    "        self.accelerator = accelerator\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.optim = self._get_optim()\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optim, T_0=5,eta_min=1e-7)\n",
    "\n",
    "        self.logger = logger\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def prepare(self):\n",
    "        self.model, self.optim, self.train_loader, self.val_loader, self.scheduler = self.accelerator.prepare(\n",
    "            self.model, \n",
    "            self.optim, \n",
    "            self.train_loader, \n",
    "            self.val_loader, \n",
    "            self.scheduler\n",
    "        )\n",
    "        \n",
    "    def _get_optim(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(self.model.classifier.parameters(), lr=self.config['lr'], eps=self.config['adam_eps'])\n",
    "        return optimizer\n",
    "\n",
    "        \n",
    "    def loss_fn(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def train_one_epoch(self,epoch,log_vars):\n",
    "\n",
    "        # statistics\n",
    "        running_corrects = 0.        \n",
    "        running_loss = 0.\n",
    "        progress = tqdm(self.train_loader, total=len(self.train_loader))\n",
    "        \n",
    "        for idx,(inputs,targets) in enumerate(progress):\n",
    "            with self.accelerator.accumulate(self.model):\n",
    "            \n",
    "                outputs = self.model(inputs)\n",
    "                _,preds = torch.max(torch.softmax(outputs,-1),-1)\n",
    "\n",
    "                loss = self.loss(outputs, targets['labels']) \n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "\n",
    "                self.accelerator.backward(loss)\n",
    "                \n",
    "                self.optim.step()                \n",
    "                \n",
    "                if self.config['enable_scheduler']:\n",
    "                    self.scheduler.step(epoch - 1 + idx / len(self.train_loader))\n",
    "\n",
    "                # statistics\n",
    "                running_corrects += torch.sum(preds == targets[\"labels\"].data)\n",
    "                    \n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                del inputs, targets, outputs, loss\n",
    "\n",
    "        train_loss = running_loss/len(self.train_loader)  \n",
    "        epoch_acc = running_corrects.double() / dataloaders[\"train\"][1]\n",
    "\n",
    "        if self.debug:\n",
    "            print(f'\"train Loss: {train_loss:.4f} Acc: {epoch_acc:.4f}\\n')\n",
    "\n",
    "        \n",
    "        log_vars.update(\n",
    "            {f\"Train/Loss\" : (epoch,train_loss),\n",
    "                                      f\"Train/Accuracy\" : (epoch,epoch_acc.cpu().numpy())\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "        self.logger.add_scalars(log_vars)\n",
    "        \n",
    "        self.train_losses.append(train_loss)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def valid_one_epoch(self,epoch,best_acc,log_vars):\n",
    "        \n",
    "        running_loss = 0.\n",
    "        running_corrects = 0.\n",
    "        progress = tqdm(self.val_loader, total=len(self.val_loader))\n",
    "        \n",
    "        for (inputs, targets) in progress:\n",
    "            \n",
    "            outputs = self.model(inputs)\n",
    "            _,preds = torch.max(torch.softmax(outputs,-1),-1)\n",
    "            \n",
    "            loss = self.loss(outputs, targets['labels'])\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # statistics\n",
    "            running_corrects += torch.sum(preds == targets[\"labels\"].data)\n",
    "            \n",
    "            del inputs, targets, outputs, loss\n",
    "\n",
    "        val_loss = running_loss/len(self.val_loader)\n",
    "        epoch_acc = running_corrects.double() / dataloaders[\"valid\"][1]\n",
    "\n",
    "        # deep copy the model\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            torch.save(self.model.state_dict(), self.best_model_params_path)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f'\"Valid Loss: {val_loss:.4f} Acc: {epoch_acc:.4f}\\n')\n",
    "\n",
    "        \n",
    "        log_vars.update(\n",
    "            {f\"Valid/Loss\" : (epoch,val_loss),\n",
    "                                      f\"Valid/Accuracy\" : (epoch,epoch_acc.cpu().numpy())\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "        self.logger.add_scalars(log_vars)\n",
    "            \n",
    "        \n",
    "        \n",
    "        self.val_losses.append(val_loss)\n",
    "        \n",
    "    \n",
    "    def test(self, test_loader):\n",
    "        \n",
    "        preds = []\n",
    "        for (inputs) in test_loader:\n",
    "            \n",
    "            outputs = self.model(inputs)\n",
    "            preds.append(outputs.detach().cpu())\n",
    "            \n",
    "        preds = torch.concat(preds)\n",
    "        return preds\n",
    "    \n",
    "    def fit(self):\n",
    "\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "        self.best_model_params_path = os.path.join(self.model_dir, 'best_model_params.pt')\n",
    "        torch.save(model.state_dict(), self.best_model_params_path)\n",
    "\n",
    "        log_vars = {}\n",
    "        \n",
    "        self.prepare()\n",
    "        \n",
    "        fit_progress = tqdm(\n",
    "            range(1, self.config['epochs']+1),\n",
    "            desc=\"Training...\"\n",
    "        )\n",
    "\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        for epoch in fit_progress:\n",
    "            if self.debug:\n",
    "                print(f\"Epoch : {epoch}\")\n",
    "            log_vars.update({\n",
    "                \"learning_rate\" : (epoch,self.scheduler.get_last_lr()[0] if self.scheduler  else self.optim.param_groups[0]['lr'])\n",
    "            }\n",
    "            )\n",
    "            \n",
    "            self.model.train()\n",
    "            fit_progress.set_description(f\"EPOCH {epoch} / {self.config['epochs']} | training...\")\n",
    "            self.train_one_epoch(epoch,log_vars)\n",
    "            self.clear()\n",
    "            \n",
    "            self.model.eval()\n",
    "            fit_progress.set_description(f\"EPOCH {epoch} / {self.config['epochs']} | validating...\")\n",
    "            self.valid_one_epoch(epoch,best_acc,log_vars)\n",
    "            self.clear()\n",
    "\n",
    "            #print(f\"{'➖️'*10} EPOCH {epoch} / {self.config['epochs']} {'➖️'*10}\")\n",
    "            #print(f\"train loss: {self.train_losses[-1]}\")\n",
    "            #print(f\"valid loss: {self.val_losses[-1]}\\n\\n\")\n",
    "            \n",
    "    \n",
    "    def clear(self):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.abspath(os.getcwd()) + \"/model/DeBERTaV3-base-FE2/\"\n",
    "log_dir = os.path.abspath(os.getcwd()) + \"/runs/DeBERTaV3-base-FE2/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "logger = TensorBoardLogger(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model,dir, (train_loader, valid_loader), config, accelerator,logger,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 1 / 30 | training...:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [19:04<00:00,  1.21it/s]\n",
      "EPOCH 1 / 30 | validating...:   0%|          | 0/30 [19:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"train Loss: 1.1214 Acc: 5.4195\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 347/347 [04:03<00:00,  1.42it/s]\n",
      "EPOCH 2 / 30 | training...:   3%|▎         | 1/30 [23:09<11:11:32, 1389.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Valid Loss: 1.0830 Acc: 5.4697\n",
      "\n",
      "Epoch : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [19:09<00:00,  1.21it/s]\n",
      "EPOCH 2 / 30 | validating...:   3%|▎         | 1/30 [42:18<11:11:32, 1389.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"train Loss: 1.0602 Acc: 5.5545\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 347/347 [04:07<00:00,  1.40it/s]\n",
      "EPOCH 3 / 30 | training...:   7%|▋         | 2/30 [46:28<10:50:56, 1394.86s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Valid Loss: 1.0765 Acc: 5.4899\n",
      "\n",
      "Epoch : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 392/1385 [05:36<14:13,  1.16it/s]\n",
      "EPOCH 3 / 30 | training...:   7%|▋         | 2/30 [52:04<12:09:09, 1562.48s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 176\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    175\u001b[0m fit_progress\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlog_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[19], line 59\u001b[0m, in \u001b[0;36mTrainer.train_one_epoch\u001b[0;34m(self, epoch, log_vars)\u001b[0m\n\u001b[1;32m     55\u001b[0m _,preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39msoftmax(outputs,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(outputs, targets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n\u001b[0;32m---> 59\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstep()                \n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_sys_fashion_mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
