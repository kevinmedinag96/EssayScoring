{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"/content/drive/MyDrive/MCC-i cuarto semestre/Data Analytics/Project/\"\n",
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"torch==2.2.2\" tensorboard\n",
    "!pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\" peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import f_oneway\n",
    "import re\n",
    "sns.set()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, cohen_kappa_score, balanced_accuracy_score, accuracy_score, roc_auc_score,r2_score\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to Hugging face (required only for Llama 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if it is desired to fine-tune Llama-3 to the Essay Scoring Dataset, then you have to log in into hugging face hub to download the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Essay Score Dataset & perfom NLP Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essay Scoring Dataset contains: 17,307 essays\n",
    "The label consists of a hollistic score from 1 to 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/train.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "data_path = \"data/test.csv\"\n",
    "df_test = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"score\",\"essay_id\"])\n",
    "y = df[\"score\"].astype(float) \n",
    "test = df_test.drop(columns=[\"essay_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntetic probabilistic distribution in the class label space used for compensating our unbalance dataset.\n",
    "Note: This distribution is only relevant if the optimization problem is formulated as a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1855, 0.1454, 0.1274, 0.1546, 0.1888, 0.1982])\n"
     ]
    }
   ],
   "source": [
    "class_weights = y.value_counts(normalize=True).sort_index().to_list()\n",
    "class_weights = torch.Tensor(class_weights)\n",
    "class_weights=(1 - class_weights) / (1 - class_weights).sum()\n",
    "print(class_weights)\n",
    "\n",
    "labels_arr = np.arange(class_weights.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a simple preprocessing to the essays:\n",
    "1.- Remove contractions\n",
    "2.- Remove HTMLs and URLs\n",
    "3.- Remove consecutive spaces, commas, and periods\n",
    "\n",
    "Note: Gramatical mistakes were preserved with the objective for the model to be aware of these errors and increase its capability to make more assertive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHTML(x):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',x)\n",
    "\n",
    "\n",
    "cList = {#\"dont\" : \"do not\", \"doesnt\" : \"does not\", \"thats\" : \"that is\"\n",
    "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "    \"he'd\": \"he would\",  ## --> he had or he would\n",
    "    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n",
    "    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",   ## --> I had or I would\n",
    "    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",   ## --> It had or It would\n",
    "    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",   ## --> It had or It would\n",
    "    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n",
    "    \"you're\": \"you are\",  \"you've\": \"you have\"\n",
    "}\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "def dataPreprocessing(x):\n",
    "    # Convert words to lowercase\n",
    "    x = x.lower()\n",
    "    # Remove HTML\n",
    "    x = removeHTML(x)\n",
    "    # Delete strings starting with @\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    # Delete Numbers\n",
    "    #x = re.sub(\"'\\d+\", '',x)\n",
    "    #x = re.sub(\"\\d+\", '',x)\n",
    "    # Delete URL\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    # Remove \\xa0\n",
    "    x = x.replace(u'\\xa0',' ')\n",
    "    # Replace consecutive empty spaces with a single space character\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = expandContractions(x)\n",
    "    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "#     x = re.sub(r'[^\\w\\s.,;:\"\"''?!]', '', x)\n",
    "  #replace \\'s with 's\n",
    "    #print(re.findall(\"\\\\'s\", x))\n",
    "    #x = re.sub(r\"\\[\\]'s\", \"'s\", x)\n",
    "    # Remove empty characters at the beginning and end\n",
    "    x = x.strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"full_text\"] = X.apply(lambda x: dataPreprocessing(x[\"full_text\"]),axis=1)\n",
    "test[\"full_text\"] = test.apply(lambda x: dataPreprocessing(x[\"full_text\"]),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>many people have car where they live. the thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am a scientist at nasa that is discussing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people always wish they had the same technolog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text\n",
       "0  many people have car where they live. the thin...\n",
       "1  i am a scientist at nasa that is discussing th...\n",
       "2  people always wish they had the same technolog..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17307"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train-Validation Pandas Dataset Split Note: Huggingface trainer needs the datasets to be compatible with huggingface dataset format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set: ceil 85% of 17, 307 samples = 14,710\n",
    "\n",
    "Validation set: 17,307 samples - training samples =  2,596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 10\n",
    "generator = np.random.RandomState(seed)\n",
    "df_size = len(X)\n",
    "train_proportion = 0.85\n",
    "validation_proportion = 0.15\n",
    "train_size = int(df_size * train_proportion)\n",
    "validation_size = df_size - train_size \n",
    "arr_train_idxs = generator.choice(np.arange(len(X)),size=[train_size,],replace=False)\n",
    "\n",
    "train = X.iloc[arr_train_idxs].reset_index(drop=True)\n",
    "train_labels = y.iloc[arr_train_idxs].reset_index(drop=True)\n",
    "validation = X[~X.index.isin(arr_train_idxs)].reset_index(drop=True)\n",
    "validation_labels = y.iloc[~y.index.isin(arr_train_idxs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_concat = pd.concat([train,train_labels],axis=1)\n",
    "validation_concat =pd.concat([validation,validation_labels],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['full_text', 'score'],\n",
       "        num_rows: 14710\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['full_text', 'score'],\n",
       "        num_rows: 2597\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pandas dataframe datasets to huggingface datasets objects...\n",
    "dataset_train = Dataset.from_pandas(train_concat)\n",
    "dataset_val = Dataset.from_pandas(validation_concat)\n",
    "dataset_test = Dataset.from_pandas(test)\n",
    "\n",
    "# Combine them into a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    'val': dataset_val\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Model for Essay Score Assesment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Hyperparameters:\n",
    "<br>max_length => Context size : 2048\n",
    "<br>d_model => Embedding dimension : 768\n",
    "\n",
    "\n",
    "Optimizer Hyperparameters:\n",
    "<br>Optimizer: AdamW\n",
    "<br>First moment coefficient (Beta1) : 0.9\n",
    "<br>Second moment coefficient (Beta2) :  0.999\n",
    "<br>lr => learning rate : 6e-5\n",
    "<br>weight_decay : 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model': \"microsoft/deberta-v3-base\",\n",
    "    'max_length': 2048, #Force it to be a nice number, a.k.a. divisible by a number powered of 2\n",
    "    'batch_size': 4, \n",
    "    'epochs': 10,\n",
    "    'lr': 6e-5,\n",
    "    \"weight_decay\" : 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing model & tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the trained models from Hugging Face using the AutoModelForSequenceClassification class.\n",
    "Tokens are procesed in the model and its output is the output layer's logits for the number of classes (Multi-Class Classification) or a single logit (Regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilized the tokenizer used to train DeBERTa V3. It consists of 128,100 unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' use_compile = True\\nif use_compile:\\n    model = torch.compile(model) '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config[\"model\"],\n",
    "    #quantization_config=quantization_config,\n",
    "    num_labels=1#class_weights.shape[0]\n",
    ")\n",
    "\n",
    "\"\"\" use_compile = True\n",
    "if use_compile:\n",
    "    model = torch.compile(model) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word Embedding Lookup Table's parameters were freezed and we only allow for the context pooler layer and output layer to be trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "  \n",
    "for param in model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model\"], add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the tokenizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in 1976 nasa\\'s viking 1 took a photo of a landform that resembled a face. this excited the public. nasa has gone back to that place twice and taken higer defenition photos to make sure that it is not a face or some monument made by aliens. they have fond that it is a common landform, a mesa or butte. if it were more, nasa would tell the public right away. news of life on mars would excite the public, and provide nasa with funding for future explorations. the first photo of the landform was taken in 1976 by viking 1. each pixel in the photo represents 43 meaters. it is difficult to see deitalies in this photo. nasa believed that it was thier duty to make sure this was not a sign of extraterestrial life. in 1998, mars orbiter camera took a picture ten times sharper than the original. this picture showed more deatail, but the public was not satisfied. in 2001 nasa took another photo of the spot. in this photo a pixel is equal to 1.56 meters. this image is so sharp that if anything unusual were there, you could see it. mesas are a common landform in that area. cydonia, where the face is located has lots of other landforms like mesas and buttes. simaler landforms are also found on earth. landforms come in all different shapes and sizes, and this one happens to resemble a face. that is not unusual, we like to find thing that look like other things. for example you might see a cloud and say it looks like a bunny. likewise we see a landform and say it resembles a face. the face, \"a huge rock formation.resembles a human head\", but this is pure coincedence. the shape of the formation forms shadows giving the illusion of eyes, a nose, and a mouth. nasa has taken very detailed photographs to make sure there is not alien life. the face on mars is a mesa or butte, a common formation. if there was life on mars nasa would not hide it, it would excite people about space and provide money for future explorations.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train[\"full_text\"][1]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.encode(text)\n",
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] in 1976 nasa\\'s viking 1 took a photo of a landform that resembled a face. this excited the public. nasa has gone back to that place twice and taken higer defenition photos to make sure that it is not a face or some monument made by aliens. they have fond that it is a common landform, a mesa or butte. if it were more, nasa would tell the public right away. news of life on mars would excite the public, and provide nasa with funding for future explorations. the first photo of the landform was taken in 1976 by viking 1. each pixel in the photo represents 43 meaters. it is difficult to see deitalies in this photo. nasa believed that it was thier duty to make sure this was not a sign of extraterestrial life. in 1998, mars orbiter camera took a picture ten times sharper than the original. this picture showed more deatail, but the public was not satisfied. in 2001 nasa took another photo of the spot. in this photo a pixel is equal to 1.56 meters. this image is so sharp that if anything unusual were there, you could see it. mesas are a common landform in that area. cydonia, where the face is located has lots of other landforms like mesas and buttes. simaler landforms are also found on earth. landforms come in all different shapes and sizes, and this one happens to resemble a face. that is not unusual, we like to find thing that look like other things. for example you might see a cloud and say it looks like a bunny. likewise we see a landform and say it resembles a face. the face, \"a huge rock formation.resembles a human head\", but this is pure coincedence. the shape of the formation forms shadows giving the illusion of eyes, a nose, and a mouth. nasa has taken very detailed photographs to make sure there is not alien life. the face on mars is a mesa or butte, a common formation. if there was life on mars nasa would not hide it, it would excite people about space and provide money for future explorations.[SEP]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded) \n",
    "decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model tokenized dataset from huggingface dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c53ea7f9c9040f6a69a71bb5c5e0456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e66014122b24eb18588e612f7f7fac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['full_text'], truncation=True, max_length=config[\"max_length\"])\n",
    "\n",
    "cols_to_delete = [\"full_text\"]\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True,remove_columns=cols_to_delete)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"score\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Model Forward pass & Quadratic Weighted Kappa score test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed7f5a26b02488ca7f6186397fd743b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd3b9d1670a4d91900edec43e76291d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pad_tokenize_function(examples):\n",
    "    return tokenizer(examples['full_text'], truncation=True,padding=\"max_length\", max_length=config[\"max_length\"])\n",
    "\n",
    "cols_to_delete = [\"full_text\"]\n",
    "\n",
    "tokenized_datasets_pad = dataset.map(pad_tokenize_function, batched=True,remove_columns=cols_to_delete)\n",
    "tokenized_datasets_pad = tokenized_datasets_pad.rename_column(\"score\", \"label\")\n",
    "tokenized_datasets_pad.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenized_datasets_pad[\"train\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input_ids\" : torch.tensor([sample.tolist() for sample in batch[\"input_ids\"]],dtype= torch.long).to(device),\n",
    " \"token_type_ids\" : torch.tensor([sample.tolist() for sample in batch[\"token_type_ids\"]],dtype= torch.long).to(device),\n",
    " \"attention_mask\" : torch.tensor([sample.tolist() for sample in batch[\"attention_mask\"]],dtype= torch.long).to(device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1224],\n",
       "        [0.0969],\n",
       "        [0.1041],\n",
       "        [0.0978],\n",
       "        [0.1238],\n",
       "        [0.0992],\n",
       "        [0.0964],\n",
       "        [0.0998],\n",
       "        [0.1214],\n",
       "        [0.0990]], device='cuda:0', grad_fn=<CompiledFunctionBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs[\"logits\"][:,0].clip(0,class_weights.shape[0] - 1).round().detach().cpu()\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 3., 1., 3., 3., 4., 4., 1., 3.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = batch[\"label\"]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y1=logits,y2=labels,labels=labels_arr,weights=\"quadratic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data collator prepares batches of data for training or inference in machine learning, ensuring uniform formatting and adherence to model input requirements. This is especially crucial for variable-sized inputs like text sequences.\n",
    "\n",
    "Functions of Data Collator\n",
    "\n",
    "1.- Padding: Uniformly pads sequences to the length of the longest sequence using a special token, allowing simultaneous batch processing.\n",
    "\n",
    "2.- Batching: Groups individual data points into batches for efficient processing.\n",
    "\n",
    "3.- Handling Special Tokens: Adds necessary special tokens to sequences.\n",
    "\n",
    "4.- Converting to Tensor: Transforms data into tensors, the required format for machine learning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer,max_length=config[\"max_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics selected from the scikitlearn library: R2 and Quadratic Weighted Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #print(f\"predictions : {predictions}\")\n",
    "    #print(f\"labels : {labels}\")\n",
    "    predictions = predictions[:,0]\n",
    "\n",
    "    r2 = r2_score(labels,predictions)\n",
    "    \n",
    "    predictions = predictions.clip(1,class_weights.shape[0] ).round()\n",
    "\n",
    "    #print(f\"predictions : {predictions}\")\n",
    "    \n",
    "    return {'R2' : r2 , \n",
    "            'QWK' : cohen_kappa_score(y1=predictions,y2=labels,labels=labels_arr,weights=\"quadratic\")}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomTrainer class inherits from the Hugging Face Trainer class, overriding the compute_loss function to compute the required loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = class_weights\n",
    "            if not isinstance(self.class_weights, torch.FloatTensor):\n",
    "                self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "            self.class_weights = self.class_weights.to(device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')[:,0]\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            raise NotImplementedError(\"Class weights is deprecated in Regression problems\")\n",
    "\n",
    "        else:\n",
    "            loss = F.mse_loss(logits,labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'tests/DeBERTaV3-No-Qlora-2', \n",
    "    logging_dir = \"tests/runs/Experiment_2_regression\",  \n",
    "    learning_rate = config[\"lr\"],\n",
    "    per_device_train_batch_size = config[\"batch_size\"],\n",
    "    per_device_eval_batch_size = config[\"batch_size\"],\n",
    "    num_train_epochs = config[\"epochs\"],\n",
    "    weight_decay = config[\"weight_decay\"],\n",
    "    evaluation_strategy = 'epoch', \n",
    "    #eval_steps=2,\n",
    "    save_strategy = 'epoch',\n",
    "    #save_steps = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    logging_strategy = \"epoch\",\n",
    "    #logging_steps = 500,\n",
    "    data_seed = seed,\n",
    "    metric_for_best_model=\"QWK\",\n",
    "    save_total_limit=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    #class_weights=class_weights,\n",
    "    #callbacks= [EarlyStoppingCallback(early_stopping_patience=5,early_stopping_threshold=.001),]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36780' max='36780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36780/36780 1:09:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>R2</th>\n",
       "      <th>Qwk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.453200</td>\n",
       "      <td>0.413789</td>\n",
       "      <td>0.615440</td>\n",
       "      <td>0.715576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.447500</td>\n",
       "      <td>0.432696</td>\n",
       "      <td>0.597869</td>\n",
       "      <td>0.710259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.446200</td>\n",
       "      <td>0.410793</td>\n",
       "      <td>0.618225</td>\n",
       "      <td>0.719947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = trainer.train(\"./tests/DeBERTaV3-No-Qlora/checkpoint-25746\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_sys_fashion_mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
