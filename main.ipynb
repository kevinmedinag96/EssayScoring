{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import f_oneway\n",
    "import re\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Essay Score Dataset & perfom NLP Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/train.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "data_path = \"data/test.csv\"\n",
    "df_test = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"score\"])\n",
    "y = df[\"score\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:59: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:64: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:59: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:64: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_37408\\2985774794.py:59: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  x = re.sub(\"@\\w+\", '',x)\n",
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_37408\\2985774794.py:64: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  x = re.sub(\"http\\w+\", '',x)\n"
     ]
    }
   ],
   "source": [
    "def removeHTML(x):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',x)\n",
    "\n",
    "\n",
    "cList = {#\"dont\" : \"do not\", \"doesnt\" : \"does not\", \"thats\" : \"that is\"\n",
    "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "    \"he'd\": \"he would\",  ## --> he had or he would\n",
    "    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n",
    "    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",   ## --> I had or I would\n",
    "    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",   ## --> It had or It would\n",
    "    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",   ## --> It had or It would\n",
    "    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n",
    "    \"you're\": \"you are\",  \"you've\": \"you have\"\n",
    "}\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "def dataPreprocessing(x):\n",
    "    # Convert words to lowercase\n",
    "    x = x.lower()\n",
    "    # Remove HTML\n",
    "    x = removeHTML(x)\n",
    "    # Delete strings starting with @\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    # Delete Numbers\n",
    "    #x = re.sub(\"'\\d+\", '',x)\n",
    "    #x = re.sub(\"\\d+\", '',x)\n",
    "    # Delete URL\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    # Remove \\xa0\n",
    "    x = x.replace(u'\\xa0',' ')\n",
    "    # Replace consecutive empty spaces with a single space character\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = expandContractions(x)\n",
    "    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "#     x = re.sub(r'[^\\w\\s.,;:\"\"''?!]', '', x)\n",
    "  #replace \\'s with 's\n",
    "    #print(re.findall(\"\\\\'s\", x))\n",
    "    #x = re.sub(r\"\\[\\]'s\", \"'s\", x)\n",
    "    # Remove empty characters at the beginning and end\n",
    "    x = x.strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"full_text\"] = X.apply(lambda x: dataPreprocessing(x[\"full_text\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns=[\"essay_id\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Llama-3 using PERFT QLORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from ipywidgets) (8.22.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kevin\\miniconda3\\envs\\neural_sys_fashion_mnist\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d95337f07704135b7bd45ea08e036b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kaggle credentials set.\n",
      "Kaggle credentials successfully validated.\n"
     ]
    }
   ],
   "source": [
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/metaresearch/llama-3/pyTorch/8b/1/download...\n",
      "100%|██████████| 11.9G/11.9G [06:29<00:00, 32.8MB/s]  \n",
      "Extracting model files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model files: C:\\Users\\kevin\\.cache\\kagglehub\\models\\metaresearch\\llama-3\\pyTorch\\8b\\1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"metaresearch/llama-3/pyTorch/8b\")\n",
    "\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_LLAMA3 = \"C:/Users/kevin/.cache/kagglehub/models/metaresearch/llama-3/pyTorch/8b/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config = config.Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt Essay Scoring DataFrame to be compatible with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Optional, Union, Any\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def define_tokenizer(cfg):\n",
    "    \"\"\"\n",
    "    Let's use basic AutoTokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.architecture[\"backbone\"], trust_remote_code=True)\n",
    "\n",
    "    # Make sure that we have a pad token and and eos token that will be used for pooling\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"Setting new pad token\")\n",
    "        # pad token is missig        \n",
    "        tokenizer.pad_token=\"<|reserved_special_token_0|>\"\n",
    "        \n",
    "    if tokenizer.eos_token is None:\n",
    "        print(\"Setting new eos_token token\")\n",
    "        # eos_token token is missig\n",
    "        tokenizer.eos_token=\"<|reserved_special_token_1|>\"\n",
    "    \n",
    "    # Make sure that padding is always \"right\"\n",
    "    if tokenizer.padding_side != \"right\":\n",
    "        print(f\"Changing padding side from {tokenizer.padding_side} to 'right'\")\n",
    "        tokenizer.padding_side = \"right\"\n",
    "    return tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_sys_fashion_mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
